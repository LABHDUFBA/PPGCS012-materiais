{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semana 7 e 8 - Python\n",
    "\n",
    "Web Scraping com Python e BeautifulSoup\n",
    "\n",
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## O que é Web Scraping?\n",
    "\n",
    "> “Scraping, to state this quite formally, is a prominent technique for the automated collection of online data. (...) Scrapers, to say it more informally, are bits of software code that makes it possible to automatically download data from the Web, and to capture some of the large quantities of data about social life that are available on online platforms like Google, Twitter and Wikipedia.” (Marres & Weltevrede, 2013, p. 313)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A raspagem de dados na Web é uma técnica de coleta de informação estruturada. \n",
    "\n",
    ">[três fases] 1) website analysis, 2) website crawling, and 3) data organization. Each phase requires one to understand several Web technologies and at least one popular programming language, such as R or Python. (Krotov et al., 2020, p. 540)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entretanto, não deve ser entendida apenas como uma técnica, mas também como uma maneira particular de lidar com informação e conhecimento: “também é uma prática de análise” (Marres & Weltevrede, 2013, p. 317). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quando usar:\n",
    "\n",
    "* Quando precisamos coletar um volume grande de dados da internet\n",
    "## Quando não usar:\n",
    "\n",
    "* Quando temos uma forma mais simples de obter os dados (API, base de dados, etc.)\n",
    "* Quando os termos de uso do site não nos permitem fazer isso\n",
    "* Quando o robots.txt do site não nos permite fazer isso\n",
    "* Quando houver risco de derrubar ou comprometer a estabilidade do site\n",
    "* Quando as informações do site não são públicas\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Identificar os elementos dos site\n",
    "\n",
    "O que vc pretende raspar e como vai fazer isso? \n",
    "\n",
    "Como vai lidar com a página a ser raspada?\n",
    "\n",
    "Qual estrutura da página? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Você sabe o que é um documento HTML?\n",
    "\n",
    "* ver slides 19 a 24 [aqui](/home/ebn/Documentos/Github/development/ppgcs012-python/sem_7-8/Workshop__Web_Scraping_em_R.pdf)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vamos inspecionar o site em nosso navegador\n",
    "\n",
    "1. Acesse a página [https://pythonscraping.com/pages/page1.html](https://pythonscraping.com/pages/page1.html) \n",
    "2. Aperte o seguitne comando: `Ctrl + Shift + i`\n",
    "\n",
    "O que acontece?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ex01](img/w_s_ex1.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ex02](img/w_s_ex2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exemplo de html:\n",
    "\n",
    "```{html}\n",
    "<html>\n",
    "<head>\n",
    "    <title>PPGCS012 - Python</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Aula de Web Scraping com Python</h1>\n",
    "    <p>Na aula de hoje vamos aprender o básico sobre HTML.</p>\n",
    "    <p style=\"color: red;\">Vamos começar!</p>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Que tal colocarmos uma página online para raspar?\n",
    "\n",
    "1. escrever um arquivo de texto com o conteúdo da página e salvar em uma pasta com o nome do site como `index.html`\n",
    "2. acessar [https://app.netlify.com/drop](https://app.netlify.com/drop).\n",
    "3. arrastar a pasta pra lá.\n",
    "\n",
    "Pronto! Nossa página está no online."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Urllib e Urlopen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_urllib_ é uma biblioteca padrão do Python para requisitar dados da web.\n",
    "\n",
    "O módulo `request` da biblioteca `urllib` é responsável por fazer requisições HTTP.\n",
    "\n",
    "A função `urlopen` da biblioteca `urllib` é responsável por abrir uma página da web."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.request import urlopen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# criar uma variável com o endereço da página\n",
    "url = 'https://pythonscraping.com/pages/page1.html'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# abrir a página com a função urlopen\n",
    "html = urlopen(url)\n",
    "# ler a página com o método .read()\n",
    "page_html = html.read()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(page_html)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A função urlopen abriu a url e com o método read() retornou o conteúdo da página.\n",
    "\n",
    "Entretando para lidarmos de forma complexa com a página, precisamos de recursos que nos permitam analisar seu conteúdo.\n",
    "\n",
    "Aí que entra a biblioteca BeautifulSoup."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# instalar a biblioteca beautifulsoup4\n",
    "!pip3 install beautifulsoup4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importar bibliotecas\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# instalar a biblioteca lxml \n",
    "# essa tem vantagens em relação ao html.parser (que é incluído por padrão):\n",
    "# melhor para páginas mais complexas e com html confuso, mais rápido\n",
    "!pip3 install lxml"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# precisamos criar um objeto BeautifulSoup\n",
    "# passamos à função BeautifulSouop o conteúdo da página e o tipo de parser\n",
    "bs = BeautifulSoup(html.read(), 'lxml')  # lxml é o parser que vai ler o html"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imprimir a página\n",
    "print(bs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# para imprimir uma tag específica do html acrescentamos . e o nome da tag\n",
    "# ex: para imprimir o título bs.h1\n",
    "print(bs.h1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## find e find_all"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url = 'https://pythonscraping.com/pages/warandpeace.html'\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html, 'lxml')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# preciso encontrar todos os elementos com o nome 'span' e class 'green'\n",
    "# para isso, uso o método find_all()\n",
    "name_list = bs.find_all('span', {'class': 'green'})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(name_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imprimir o texto de cada elemento encontrado\n",
    "for name in name_list:\n",
    "    print(name.text)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "quotes_list = bs.find_all('span', {'class': 'red'})\n",
    "for quote in quotes_list:\n",
    "    print(quote.text+'\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exemplo Anpuh"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url_anpuh = \"https://anpuh.org.br/index.php/documentos/anais\"\n",
    "html_anpuh = urlopen(url_anpuh)\n",
    "bs_anpuh = BeautifulSoup(html_anpuh, 'lxml')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encontrar o box de conteúdo\n",
    "box_conteudo = bs_anpuh.find(id='cobalt-section-1')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encontrar todos os boxes dos Eventos\n",
    "eventos = box_conteudo. find_all(class_='span3 category-box')\n",
    "print(len(eventos))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encontrar h4 em cada envento\n",
    "for evento in eventos:\n",
    "    title = evento.find('h4')\n",
    "    link = title.find('a')['href']\n",
    "    print(link+'\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encontrar h4 em cada envento\n",
    "for evento in eventos:\n",
    "    try:\n",
    "        title = evento.find('h4')\n",
    "        link = title.find('a')['href']\n",
    "        print(link+'\\n')\n",
    "    # caso não encontre o h4, pula para o próximo evento\n",
    "    except:\n",
    "        print('erro')\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encontrar h4 em cada envento\n",
    "for evento in eventos:\n",
    "    try:\n",
    "        title = evento.find('h4')\n",
    "        link = title.find('a')['href']\n",
    "        full_link = 'https://anpuh.org.br'+link\n",
    "        print(full_link+'\\n')\n",
    "    # caso não encontre o h4, pula para o próximo evento\n",
    "    except:\n",
    "        print('erro')\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}